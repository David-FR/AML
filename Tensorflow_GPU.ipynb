{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0ce2c99-2b55-4e9a-814b-f4018f620391",
   "metadata": {},
   "source": [
    "# TensorFlow GPU Test\n",
    "This file is to check that the GPUs are being used by *Tensorflow 1.x* correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "956b53c2-ad9b-4505-b6dc-5055756fd178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify visible cuda device\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ea0e81d-beef-4df7-bc73-9abd37a5486a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.6\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.platform import build_info as tf_build_info\n",
    "print(tf_build_info.cudnn_version_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "170e233a-b86c-4cd0-8810-799d9ae87160",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18bffca3-72cc-4094-b3ea-eec930c09387",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:1 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0\n",
      "/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32988834-7cdc-4f74-ba16-325268d1d1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF will allocate all available memory on each visible GPU if not told otherwise. \n",
    "# Here are 5 ways to stick to just one (or a few) GPUs.: https://stackoverflow.com/questions/40069883/how-to-set-specific-gpu-in-tensorflow\n",
    "config=tf.ConfigProto(device_count={'GPU':1})\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7b52d6d-11d5-4c0d-a726-f8f515b5a77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c2887a8-5b0d-49e2-acdf-b079d6cbcca2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.15.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "GPUs available: 2\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Check TensorFlow version\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "\n",
    "# Step 2: List available GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "    try:\n",
    "        # Step 3: Set memory growth to avoid memory overflow\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        # If you want to use specific GPUs, you can specify them here:\n",
    "        # tf.config.experimental.set_visible_devices(gpus[0:2], 'GPU')\n",
    "        print(f\"GPUs available: {len(gpus)}\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before initializing GPUs\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPUs found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90511f02-eece-40c1-923e-3d05e1a5c511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11a89894-13a4-4a96-8253-22a1a33ad725",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d9a056a-55f2-42fc-8156-e0508fa40a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "7.6\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.platform import build_info as tf_build_info\n",
    "print(tf_build_info.cuda_version_number)\n",
    "# 9.0 in v1.10.0\n",
    "print(tf_build_info.cudnn_version_number)\n",
    "# 7 in v1.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bbcaa04-f981-416d-ab4e-7e2177d9ad46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA {'home': '/software/spackages/linux-rocky8-x86_64/gcc-9.5.0/cuda-11.1.1-gvdep34zxhk3vvzcg3dti7ytgmuf5547', 'include': '/software/spackages/linux-rocky8-x86_64/gcc-9.5.0/cuda-11.1.1-gvdep34zxhk3vvzcg3dti7ytgmuf5547/include', 'lib64': '/software/spackages/linux-rocky8-x86_64/gcc-9.5.0/cuda-11.1.1-gvdep34zxhk3vvzcg3dti7ytgmuf5547/lib64'}\n",
      "CUDA_VERSION 11.1\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "IS_WINDOWS = False\n",
    "\n",
    "def get_cuda_version(cuda_home):\n",
    "    \"\"\"Locate the CUDA version\n",
    "    \"\"\"\n",
    "    version_file = os.path.join(cuda_home, \"version.txt\")\n",
    "    try:\n",
    "        if os.path.isfile(version_file):\n",
    "            with open(version_file) as f:\n",
    "                version_str = f.readline().replace('\\n', '').replace('\\r', '')\n",
    "                return version_str.split(\" \")[2][:4]\n",
    "        else:\n",
    "            version_str = subprocess.check_output([os.path.join(cuda_home,\"bin\",\"nvcc\"),\"--version\"])\n",
    "            version_str=str(version_str).replace('\\n', '').replace('\\r', '')\n",
    "            idx=version_str.find(\"release\")\n",
    "            return version_str[idx+len(\"release \"):idx+len(\"release \")+4]\n",
    "    except:\n",
    "        raise RuntimeError(\"Cannot read cuda version file\") \n",
    "def locate_cuda():\n",
    "    \"\"\"Locate the CUDA environment on the system\n",
    "\n",
    "    Returns a dict with keys 'home', 'include' and 'lib64'\n",
    "    and values giving the absolute path to each directory.\n",
    "\n",
    "    Starts by looking for the CUDA_HOME or CUDA_PATH env variable. If not found, everything\n",
    "    is based on finding 'nvcc' in the PATH.\n",
    "    \"\"\"\n",
    "    # Guess #1\n",
    "    cuda_home = os.environ.get('CUDA_HOME') or os.environ.get('CUDA_PATH')\n",
    "    if cuda_home is None:\n",
    "        # Guess #2\n",
    "        try:\n",
    "            which = 'where' if IS_WINDOWS else 'which'\n",
    "            nvcc = subprocess.check_output(\n",
    "                [which, 'nvcc']).decode().rstrip('\\r\\n')\n",
    "            cuda_home = os.path.dirname(os.path.dirname(nvcc))\n",
    "        except subprocess.CalledProcessError:\n",
    "            # Guess #3\n",
    "            if IS_WINDOWS:\n",
    "                cuda_homes = glob.glob(\n",
    "                    'C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v*.*')\n",
    "                if len(cuda_homes) == 0:\n",
    "                    cuda_home = ''\n",
    "                else:\n",
    "                    cuda_home = cuda_homes[0]\n",
    "            else:\n",
    "                cuda_home = '/usr/local/cuda'\n",
    "            if not os.path.exists(cuda_home):\n",
    "                cuda_home = None\n",
    "    version = get_cuda_version(cuda_home)\n",
    "    cudaconfig = {'home': cuda_home,\n",
    "                  'include': pjoin(cuda_home, 'include'),\n",
    "                  'lib64': pjoin(cuda_home, pjoin('lib', 'x64') if IS_WINDOWS else 'lib64')}\n",
    "    if not all([os.path.exists(v) for v in cudaconfig.values()]):\n",
    "        raise EnvironmentError(\n",
    "            'The CUDA  path could not be located in $PATH, $CUDA_HOME or $CUDA_PATH. '\n",
    "            'Either add it to your path, or set $CUDA_HOME or $CUDA_PATH.')\n",
    "\n",
    "    return cudaconfig, version\n",
    "\n",
    "\n",
    "CUDA, CUDA_VERSION = locate_cuda()\n",
    "print('CUDA', CUDA)\n",
    "print('CUDA_VERSION', CUDA_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86749401-1a31-4f51-b9ff-794f4bb37c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22. 28.]\n",
      " [49. 64.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "with tf.device('/gpu:1'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print (sess.run(c))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv3",
   "language": "python",
   "name": "myenv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
